{
  "name": "Wikipedia Philosophy Challenge",
  "tagline": "The \"Getting to Philosophy\" Challenge with Python and MongoDB",
  "body": "\r\n\r\n#### Overview: The \"Getting to Philosophy\" Challenge\r\n\r\nCheck out the wikipedia page on this: https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy\r\n\r\n> Clicking on the first link in the main text of a Wikipedia article, and then repeating the process for subsequent articles, usually eventually gets you to the Philosophy article. As of May 26, 2011, 94.52% of all articles in Wikipedia lead eventually to the article Philosophy. The remaining 100,000 (approx.) links to an article with no wikilinks or with links to pages that do not exist, or get stuck in loops (all three are equally probable). The median link chain length to reach philosophy is 23.\r\n\r\nIf you go to any page on Wikipedia and keep clicking on the first link of the page (ignoring links in parenthesis and the ones in italic), you will usually eventually reach the Philosophy page. This is a python implementation of the challenge that uses [MongoDB](https://mlab.com) to cache previously visited pages in order to make aggregating results easier and to reduce the overall number of requests to Wikipedia.\r\n\r\nMy stats differ from the main article, which is most likely due to slightly different assumptions about what the \"first link\" on the page should be. (E.g. some implementations ignore external links, etc.). As of this writing, this implementation finds 94.2% articles lead to the Philosophy page with a median link chain of 14.\r\n\r\n\r\n#### Installation\r\n\r\nBest to start with a fresh virtual environment,\r\n\r\n```bash\r\n$ virtualenv env\r\nNew python executable in env/bin/python\r\nInstalling setuptools, pip, wheel...done.\r\n```\r\n\r\nDon't forget activate and then install all dependencies,\r\n\r\n```bash\r\n$ source env/bin/activate\r\n(env)$ pip install -r requirements.txt\r\n```\r\n\r\nMake the scripts executable and take a look at the help menu,\r\n\r\n```bash\r\n(env)$ chmod +x results.py crawler.py\r\n(env)$ ./crawler.py -h\r\nusage: crawler.py [-h] -u URL [-l LIMIT] [-n RUNS] [-m MONGO] [-i]\r\n\r\nWikipedia philosophy crawler\r\n\r\noptional arguments:\r\n  -h, --help  show this help message and exit\r\n  -u URL      the starting url\r\n  -l LIMIT    page depth limit\r\n  -n RUNS     run <n> times\r\n  -m MONGO    mongo credentials\r\n  -i          ignore caching\r\n```\r\n\r\n#### Crawler\r\n\r\nThe bulk of the code is in crawler.py. To see the basic algorithm at work, run the following:\r\n\r\n```bash\r\n(env)$ ./crawler.py -u Art -i\r\n{ 'errors': False,\r\n  'message': 'Found in cache',\r\n  'path_link_limit': 40,\r\n  'reaches_philosophy': True,\r\n  'starting_url': 'Art',\r\n  'urls': ['Art',\r\n          'Human_behavior',\r\n          'Motion_(physics)',\r\n          'Physics',\r\n          'Natural_science',\r\n          'Science',\r\n          'Knowledge',\r\n          'Awareness',\r\n          'Conscious',\r\n          'Quality_(philosophy)',\r\n          'Philosophy']}\r\n```\r\n\r\nSignificant speed improvements can be made by utilizing previously successful urls. Remove the '-i' to see this at work:\r\n\r\n```bash\r\n(env)$ ./crawler.py -u Art\r\nRunning with 1448 successful_urls\r\n{ 'errors': False,\r\n  'message': 'Found in cache',\r\n  'path_link_limit': 40,\r\n  'reaches_philosophy': True,\r\n  'starting_url': 'Art',\r\n  'urls': [u'Art',\r\n          u'Human_behavior',\r\n          u'Motion_(physics)',\r\n          u'Physics',\r\n          u'Natural_science',\r\n          u'Science',\r\n          u'Knowledge',\r\n          u'Awareness',\r\n          u'Conscious',\r\n          u'Quality_(philosophy)',\r\n          u'Philosophy']}\r\n```\r\n\r\nI've stored previously successful urls to a (free!) [MongoDB](https://mlab.com) instance. In order to generate the data stored there, I ran the following a few times between coffee breaks:\r\n\r\n```bash\r\n(env)$ ./crawler.py -u Special:Random -m <admin_username>:<admin_password> -n 50\r\n```\r\n\r\nYou will need to set up your own MongoDB instance in order to cache previous successful urls or use results.py to see the aggregate results of the challenge. I recommend setting up two users: one with write permissions and one read-only user. You can change the <username>:<password> from the command like with \"-m\". This defaults to \"read:read\" when not specified (which was my \"read-only\" username/password for this exercise).\r\n\r\n\r\n#### Results\r\n\r\nThe data stored on the mongo instance should be all that's necessary to analyze the results. So I wrote a separate, smaller script to do just that:\r\n\r\n```bash\r\n(env)$ ./results.py\r\nTotal number of pages: 499\r\n\r\nPercentage of pages that often lead to philosophy? 94.2%\r\n\r\nWhat is the distribution of path lengths, discarding those paths that never reach the Philosophy page?\r\nSize:  470\r\nMean: 14.32\r\nMedian: 14\r\nStd: 4.04\r\n```\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}